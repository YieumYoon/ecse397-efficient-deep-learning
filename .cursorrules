# CWRU Markov GPU Cluster Guidelines

## Official Documentation Source
These rules are based on official CWRU HPC documentation.
DO NOT trust existing scripts in this repository as reference - they may not follow best practices.

## Cluster Access
- **Markov cluster hostname**: `markov.case.edu` (for coursework)
- **Access methods**: OnDemand web portal, X2Go, or SSH terminals
- **VPN Required**: Must use CWRU VPN when accessing from outside campus network
- **Interactive session keep-alive**: Use `ssh -o ServerAliveInterval=60` to prevent disconnection

## Critical Rules - Job Execution

### DO NOT Run on Login Nodes
NEVER run computational jobs directly on login nodes. Always use:
- **Batch jobs**: `sbatch script.slurm`
- **Interactive jobs**: `srun --x11 --pty bash`

### Storage Location Requirements
**CRITICAL**: Jobs MUST run on scratch space, NOT home directory:
- **Markov GPU nodes**: Use `$TMPDIR` as scratch space (automatically set by SLURM)
- **Alternative**: Use `$PFSDIR` as scratch space (also available)
- **Do NOT use**: `/mnt/fs1` (permission denied)
- Copy input data to scratch at job start
- Copy results back to home before job ends
- `$TMPDIR` is automatically cleaned up by SLURM after job completion

## SLURM Job Submission

### Partition Selection
For Markov cluster coursework:
- **CPU jobs**: Use `--partition=markov_cpu` (default partition)
- **GPU jobs**: Use `--partition=markov_gpu` (verified available)
- Other partitions: `batch`, `smp`, `gpu`
- Use node features with `-C` flag if needed

**Verified partitions on markov.case.edu:**
- `markov_cpu*` (default, CPU-only nodes)
- `markov_gpu` (GPU nodes with CUDA support)

### Resource Requests in SLURM Scripts
Required SBATCH directives:
```bash
#!/bin/bash
#SBATCH --job-name=job_name
#SBATCH --partition=gpu              # or batch, smp, markov_cpu
#SBATCH --gres=gpu:1                 # if using GPU
#SBATCH --cpus-per-task=N            # number of CPU cores
#SBATCH --mem=XG                     # memory request
#SBATCH --time=HH:MM:SS              # wall time limit
#SBATCH --output=/path/to/output_%j.out
#SBATCH --error=/path/to/error_%j.err
```

### Proper SLURM Script Structure
```bash
#!/bin/bash
#SBATCH [directives here]

# Error handling
set -euo pipefail

# Change to scratch space (REQUIRED)
# $TMPDIR is automatically set by SLURM for each job
WORK_DIR="$TMPDIR"
cd "$WORK_DIR"

echo "Working in: $WORK_DIR"

# Copy input data to scratch
cp -r $HOME/project/data .
cp -r $HOME/project/src .

# Load modules
module purge
module load PyTorch-bundle/2.1.2-foss-2023a-CUDA-12.1.1

# Run computation
[your commands here]

# Copy results back to home directory (CRITICAL!)
mkdir -p $HOME/project/results
cp -r results/* $HOME/project/results/

# Cleanup: TMPDIR is automatically cleaned by SLURM
echo "Job complete - TMPDIR will be auto-cleaned"
```

## Good Citizenship Practices

### Compilation
- Limit parallel compilation: `make --jobs=4` (max 4 parallel processes)
- Never use unlimited `make -j`

### Directory Management
- Keep separate directories for each job
- Use job-specific output directories
- Clean up temporary files after jobs complete

### Interactive vs Batch Jobs
- **Batch jobs** (recommended): More reliable, won't disconnect
- **Interactive jobs**: May disconnect after hours, use for debugging only
- For long-running work, always use batch submission

## Module System
- Load required modules in SLURM scripts
- Example: `module load python/3.x cuda/11.x`
- Check available modules with: `module avail`
- **Verified PyTorch module**: `PyTorch-bundle/2.1.2-foss-2023a-CUDA-12.1.1`
- Always use `module purge` before loading to ensure clean environment

## GPU Usage and Selection

### Checking GPU Availability
Use the `si` command to check current GPU status:
```bash
si  # Shows all nodes with GPU types and availability
```

**Available GPU Types on markov_gpu partition:**
- `gpu2h100` - NVIDIA H100 (best performance, 2 nodes)
- `gpu4090` - NVIDIA RTX 4090 (excellent performance)
- `gpul40s` - NVIDIA L40S (good performance)
- `gpu2080` - NVIDIA RTX 2080 Ti (most available, 14+ nodes)
- `gpu4070` - NVIDIA RTX 4070 (good performance)

### Selecting Specific GPU Types
Use `-C` flag to request specific GPU features:
```bash
#SBATCH -C gpu2h100     # Request H100 (best)
#SBATCH -C gpu4090      # Request RTX 4090
#SBATCH -C gpu2080      # Request RTX 2080 Ti (most available)
```

### Basic GPU Request
When requesting GPU resources:
- Always specify `--gres=gpu:N` where N is number of GPUs
- Use `--partition=markov_gpu` for GPU access
- Optionally specify GPU type with `-C <feature>`

### Automatic Best GPU Selection

⚠️ **CRITICAL: Always request specific GPU types to avoid slow nodes!**

Without `-C` flag, SLURM assigns random available GPUs. You might get slow nodes like:
- `classt13` (gpu2080) - slowest tier, ~2-3x slower than H100
- Training that takes 1 hour on H100 could take 2-3 hours on gpu2080

**RECOMMENDED: Use the automatic GPU selection wrapper:**
```bash
# Best practice - automatically selects fastest available GPU
bash scripts/submit_best_gpu.sh scripts/train_cnn.slurm

# Or manually specify best GPU
sbatch -C gpu2h100 scripts/train_cnn.slurm  # Request H100 specifically
sbatch -C gpu4090 scripts/train_vit.slurm   # Request 4090 if H100 busy
```

**Check GPU availability before submitting:**
```bash
si  # Shows real-time GPU status
# Look for "idle" nodes with gpu2h100, gpu4090, or gpul40s
```

**Get GPU type for manual use:**
```bash
GPU_TYPE=$(bash scripts/select_best_gpu.sh)
sbatch -C $GPU_TYPE <script.slurm>
```

**GPU Performance Comparison** (best to worst):

| GPU Type | Nodes | Performance | When to Use |
|----------|-------|-------------|-------------|
| `gpu2h100` (H100) | classt[23-24] | ⚡ **FASTEST** | Always prefer for training |
| `gpu4090` (RTX 4090) | classt25 | ⚡⚡ Excellent | If H100 busy |
| `gpul40s` (L40S) | classt[21-22] | ⚡⚡⚡ Good | Large memory tasks |
| `gpu4070` (RTX 4070) | classt06 | ⚡⚡⚡⚡ Decent | Light workloads |
| `gpu2080` (RTX 2080 Ti) | classt[01-19]* | ⚡⚡⚡⚡⚡ Slowest | Avoid for training |

*Most available (14+ nodes) but slowest - only use for testing or when nothing else available.

**Real Performance Impact:**
- ResNet-18 training: ~2 hours on H100 vs ~5 hours on gpu2080
- ViT-Tiny training: ~1 hour on H100 vs ~3 hours on gpu2080
- Inference/testing: Minimal difference, any GPU is fine

**Example: Check which GPU your job got:**
```bash
sacct -j <job_id> --format=JobID,NodeList
# If you got classt13-19, consider canceling and resubmitting with -C gpu2h100
```

See `CLUSTER_DOCUMENTATION.md` for detailed information.

## Monitoring Jobs
- Check job status: `squeue -u $USER`
- Cancel job: `scancel <job_id>`
- Check job details: `scontrol show job <job_id>`

## Common Mistakes to Avoid
1. ❌ Running jobs in home directory instead of scratch space
2. ❌ Running computational work on login nodes
3. ❌ Using unlimited parallel compilation (`make -j`)
4. ❌ Forgetting to copy results back from scratch before job ends
5. ❌ Not specifying resource requirements accurately
6. ❌ Forgetting VPN when accessing from off-campus
7. ❌ **Not requesting specific GPU types** - letting SLURM assign slow gpu2080 nodes
8. ❌ **Submitting with just `--gres=gpu:1`** - use `bash scripts/submit_best_gpu.sh` instead

## Python Environment Best Practices
- Create virtual environments in home directory
- Activate environment in SLURM script before running Python
- Copy only necessary data to scratch, not entire environment
- Use `python -u` flag for unbuffered output in batch jobs

## File Permissions and Data
- Home directory: For code, environments, small files
- Scratch space: For job execution, large datasets, temporary files
- Check disk usage: `du -sh <directory>`
- Check quotas: `quota -s`

## When Generating SLURM Scripts
Always include:
1. Proper shebang: `#!/bin/bash`
2. All required SBATCH directives
3. `set -euo pipefail` for error handling
4. `cd` to appropriate scratch space
5. Data transfer to/from scratch
6. Cleanup commands
7. Module loads if needed
8. Error and output log paths

## Testing and Validation
- Test scripts with short wall times first
- Use interactive sessions for debugging
- Verify scratch space usage and cleanup
- Check output/error logs after job completion

### Cluster Verification Test
Run the verification script to test your setup:
```bash
sbatch scripts/test_cluster_setup.slurm
```

This tests:
- Scratch space access and I/O
- Module system (PyTorch-bundle)
- Python environment
- CUDA/GPU availability and computation
- Data downloading (CIFAR-10)
- File copying to/from scratch
- Cleanup procedures

## Important Notes Based on Testing

### Environment Variables
- `$TMPDIR` and `$PFSDIR` are **only available within SLURM jobs**, not on login nodes
- `$TMPDIR` = `/tmp/job.JOBID.markov2` (unique per job, auto-created, auto-cleaned)
- `$PFSDIR` = `/scratch/markov2/jobs/job.JOBID.markov2` (alternative scratch space)
- Disk space: `$TMPDIR` has ~1.7TB available
- **Use `$TMPDIR` for scratch work** - simpler and automatically managed

### Disk Space Management
- CIFAR-10 dataset (~170MB) is automatically downloaded to scratch by PyTorch
- Pretrained models are cached in `~/.cache/torch/hub/checkpoints/` in home directory
- Only copy back essential results (model checkpoints), not datasets or temp files

### Common Pitfalls in Existing Scripts
Many existing scripts in repositories (including this one) may violate best practices:
- Running in home directory instead of scratch
- Missing data copy operations
- No scratch cleanup
- Incorrect partition names
- Missing module loads

**Always verify scripts against official documentation, not against existing code.**
